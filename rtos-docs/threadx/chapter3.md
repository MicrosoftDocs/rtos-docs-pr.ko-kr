---
title: 챕터 3 - Azure RTOS ThreadX의 기능 구성 요소
description: 이 챕터에는 기능적 관점의 고성능 Azure RTOS ThreadX 커널에 대한 설명이 포함되어 있습니다.
author: philmea
ms.author: philmea
ms.date: 05/19/2020
ms.topic: article
ms.service: rtos
ms.openlocfilehash: aa66ad392171958e5d2cc765992fd1a9e41250a6
ms.sourcegitcommit: e3d42e1f2920ec9cb002634b542bc20754f9544e
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/22/2021
ms.locfileid: "104812187"
---
# <a name="chapter-3---functional-components-of-azure-rtos-threadx"></a>챕터 3 - Azure RTOS ThreadX의 기능 구성 요소

이 챕터에는 기능적 관점의 고성능 Azure RTOS ThreadX 커널에 대한 설명이 포함되어 있습니다. 각 기능 구성 요소는 이해하기 쉽게 설명되어 있습니다.

## <a name="execution-overview"></a>실행 개요

ThreadX 애플리케이션에는 초기화, 스레드 실행, ISR(인터럽트 서비스 루틴) 및 애플리케이션 타이머라는 네 가지 프로그램 실행 형식이 있습니다.

그림 2는 각기 다른 프로그램 실행 형식을 보여줍니다. 이러한 각 형식에 대한 자세한 내용은 이 챕터의 후속 섹션을 참조하세요.

### <a name="initialization"></a>초기화

이름에서 알 수 있듯이 이것은 ThreadX 애플리케이션에서 첫 번째 프로그램 실행 형식입니다. 초기화에는 프로세서 재설정과 *스레드 예약 루프* 의 진입점 사이에 있는 모든 프로그램 실행이 포함됩니다.

### <a name="thread-execution"></a>스레드 실행

초기화가 완료되면 ThreadX가 스레드 예약 루프를 시작합니다. 예약 루프는 실행할 준비가 된 애플리케이션 스레드를 찾아봅니다. 준비된 스레드를 찾으면 ThreadX는 이 스레드로 제어권을 이전합니다. 스레드가 완료되면(또는 우선 순위가 더 높은 다른 스레드가 준비되면) 준비된 스레드 중 다음으로 우선 순위가 높은 스레드를 찾기 위해 스레드 예약 루프로 실행이 다시 이전됩니다.

스레드를 지속적으로 실행하고 예약하는 이런 프로세스는 ThreadX 애플리케이션에서 가장 일반적인 프로그램 실행 형식입니다.

### <a name="interrupt-service-routines-isr"></a>ISR(인터럽트 서비스 루틴)

인터럽트는 실시간 시스템의 초석입니다. 인터럽트가 없으면 외부 세계의 변화에 적시에 대응하기가 매우 어렵습니다. 인터럽트가 감지되면 프로세서는 현재 프로그램 실행에 대한 주요 정보를 저장한 다음(일반적으로 스택에), 미리 정의된 프로그램 영역으로 제어권을 이전합니다. 미리 정의된 프로그램 영역을 일반적으로 인터럽트 서비스 루틴이라고 합니다. 대부분의 경우 인터럽트는 스레드 실행 중에(또는 스레드 예약 루프에서) 발생합니다. 하지만 실행 중인 ISR 또는 애플리케이션 타이머 내부에서도 인터럽트가 발생할 수 있습니다.

![프로그램 실행 형식](./media/user-guide/types-program-execution.png)

**그림 2. 프로그램 실행 형식**

### <a name="application-timers"></a>애플리케이션 타이머

애플리케이션 타이머는 하드웨어 구현(일반적으로 정기적인 단일 하드웨어 인터럽트가 사용됨)이 애플리케이션에서 숨겨지는 점을 제외하면 ISR과 유사합니다. 이러한 타이머는 애플리케이션에서 시간 제한 서비스, 정기적 서비스 및/또는 감시 서비스를 수행하는 데 사용됩니다. ISR과 마찬가지로 애플리케이션 타이머는 스레드 실행을 가장 자주 중단합니다. 단, ISR과 달리 애플리케이션 타이머는 서로를 인터럽트할 수는 없습니다.

## <a name="memory-usage"></a>메모리 사용량

ThreadX는 애플리케이션과 함께 상주합니다. 그 결과, ThreadX의 정적 메모리(또는 고정 메모리) 사용량은 개발 도구(예: 컴파일러, 링커 및 로케이터)에 의해 결정됩니다. 동적 메모리(또는 런타임 메모리) 사용량은 애플리케이션에서 직접 제어합니다.

### <a name="static-memory-usage"></a>정적 메모리 사용량

대부분의 개발 도구는 애플리케이션 프로그램 이미지를 *명령어*, *상수*, *초기화된 데이터*, *초기화되지 않은 데이터*, *시스템 스택* 이라는 5가지 기본 영역으로 나눕니다. 그림 3은 이러한 메모리 영역의 예를 보여줍니다.

이것은 단지 예일 뿐이라는 것을 이해해야 합니다. 실제 정적 메모리 레이아웃은 프로세서, 개발 도구 및 기본 하드웨어에 따라 다릅니다.

명령어 영역에는 프로그램의 모든 프로세서 명령이 포함됩니다. 이 영역은 대체로 가장 크며 ROM에 있는 경우가 많습니다.

상수 영역에는 프로그램 내에서 정의되거나 참조되는 문자열을 포함하는 다양한 컴파일된 상수가 포함됩니다. 또한 이 영역에는 초기화된 데이터 영역의 "초기 복사본"이 포함됩니다. *메모리 사용량* 컴파일러의 초기화 과정에서 상수 영역의 이 부분은 RAM에 초기화된 데이터 영역을 설정하는 데 사용됩니다. 상수 영역은 일반적으로 명령어 영역 다음에 있고 ROM에 있는 경우가 많습니다.

초기화된 데이터 영역과 초기화되지 않은 데이터 영역에는 모든 전역 변수와 및 정적 변수가 포함됩니다. 이러한 영역은 항상 RAM에 있습니다.

시스템 스택은 일반적으로 초기화된 데이터 영역과 초기화되지 않은 데이터 영역 바로 다음에 설정됩니다.

시스템 스택은 컴파일러에서 초기화 중에 사용된 다음, ThreadX에서 초기화 중에 사용되며 이후에는 ISR 처리에서 사용됩니다.

![메모리 영역 예](./media/user-guide/memory-area-example.png)

**그림 3. 메모리 영역 예**

### <a name="dynamic-memory-usage"></a>동적 메모리 사용량

앞서 언급했듯이 동적 메모리 사용량은 애플리케이션에서 직접 제어합니다. 스택, 큐 및 메모리 풀과 연결된 메모리 영역 및 제어 블록은 대상의 메모리 공간 어디에나 배치할 수 있습니다. 이 기능은 다양한 형식의 실제 메모리를 쉽게 활용할 수 있기 때문에 중요합니다.

예를 들어 대상 하드웨어 환경에 빠른 메모리와 느린 메모리가 모두 있다고 가정하겠습니다. 애플리케이션에 우선 순위가 높은 스레드에 대한 추가 성능이 필요한 경우 제어 블록(TX_THREAD) 및 스택을 빠른 메모리 영역에 배치할 수 있으며, 따라서 성능이 크게 향상될 수 있습니다.

## <a name="initialization"></a>초기화

초기화 프로세스를 이해하는 것은 중요합니다. 초기 하드웨어 환경이 여기에서 설정됩니다. 또한 바로 여기에서 애플리케이션에 초기 특징이 부여됩니다.

> [!NOTE]
> *ThreadX는(가능한 경우) 전체 개발 도구의 초기화 프로세스를 활용하려고 시도합니다. 이는 나중에 새 버전의 개발 도구로 쉽게 업그레이드할 수 있게 해줍니다.*

### <a name="system-reset-vector"></a>시스템 재설정 벡터

모든 마이크로프로세서에는 재설정 논리가 있습니다. 재설정(하드웨어 또는 소프트웨어)이 발생하면 특정 메모리 위치에서 애플리케이션의 진입점 주소가 검색됩니다. 진입점이 검색된 후 프로세서는 이 위치로 제어권을 이전합니다. 애플리케이션 진입점은 네이티브 어셈블리 언어로 작성되는 경우가 많으며 일반적으로 개발 도구에서(적어도 템플릿 형태로) 제공됩니다. 경우에 따라 ThreadX에 특수 버전의 엔트리 프로그램이 제공됩니다.

### <a name="development-tool-initialization"></a>개발 도구 초기화

하위 수준 초기화가 완료되면 제어권은 개발 도구의 상위 수준 초기화로 이전됩니다. 일반적으로 여기에서 초기화된 전역 변수 및 정적 C 변수가 설정됩니다. 초기 값은 상수 영역에서 검색됩니다. 정확한 초기화 처리는 개발 도구에 따라 다릅니다.

### <a name="main-function"></a>main 함수

개발 도구 초기화가 완료되면 제어권은 사용자가 제공한 *main* 함수로 이전됩니다. 이 시점에서 애플리케이션은 이후 발생하는 상황을 제어합니다. 대부분의 애플리케이션에서 main 함수는 ThreadX에 대한 엔트리인 *tx_kernel_enter* 를 호출합니다. 하지만 애플리케이션은 ThreadX에 들어가기 전에 예비 처리(일반적으로 하드웨어 초기화용)를 수행할 수 있습니다.

> [!IMPORTANT]
> *tx_kernel_enter에 대한 호출은 반환되지 않으므로 이 다음에는 처리를 배치하지 마십시오.*

### <a name="tx_kernel_enter"></a>tx_kernel_enter

엔트리 함수는 다양한 내부 ThreadX 데이터 구조의 초기화를 조정한 다음, 애플리케이션의 정의 함수인 ***tx_application_define*** 을 호출합니다.

***tx_application_define*** 이 반환되면 제어권은 스레드 예약 루프로 이전됩니다. 그러면 초기화의 끝이 표시됩니다.

### <a name="application-definition-function"></a>애플리케이션 정의 함수

***tx_application_define*** 함수는 모든 초기 애플리케이션 스레드, 큐, 세마포, 뮤텍스, 이벤트 플래그, 메모리 풀, 타이머를 정의합니다. 애플리케이션이 정상적으로 작동하는 동안 스레드에서 시스템 리소스를 만들고 삭제할 수도 있습니다. 하지만 모든 초기 애플리케이션 리소스가 여기에 정의됩니다.

***tx_application_define** _ 함수에는 단일 입력 매개 변수가 있습니다. _first-available* RAM 주소는 이 함수에 대한 유일한 입력 매개 변수입니다. 일반적으로 스레드 스택, 큐, 메모리 풀의 초기 런타임 메모리 할당을 위한 시작점으로 사용됩니다.

> [!NOTE]
> *초기화가 완료된 후에는 실행 스레드에서만 다른 스레드를 포함하는 시스템 리소스를 만들고 삭제할 수 있습니다. 따라서 초기화하는 동안 하나 이상의 스레드를 만들어야 합니다.*

### <a name="interrupts"></a>인터럽트

인터럽트는 전체 초기화 프로세스 동안 사용하지 않도록 설정된 상태로 유지됩니다. 애플리케이션에서 인터럽트를 사용하도록 설정하면 예기치 않은 동작이 발생할 수 있습니다. 그림 4는 시스템 재설정부터 애플리케이션별 초기화까지 전체 초기화 프로세스를 보여줍니다.

## <a name="thread-execution"></a>스레드 실행

애플리케이션 스레드 예약 및 실행은 ThreadX의 가장 중요한 작업입니다. 스레드는 일반적으로 전용 용도가 있는 반독립적인 프로그램 세그먼트로 정의됩니다. 모든 스레드의 처리를 결합하여 애플리케이션이 만들어집니다.

스레드는 초기화 중에 또는 스레드 실행 중에 ***tx_thread_create** _를 호출하여 동적으로 생성됩니다. 스레드는 준비됨(_ready*) 또는 일시 중단됨(*suspended*) 상태로 생성됩니다.

![초기화 프로세스](./media/user-guide/initialization-process.png)

**그림 4. 초기화 프로세스**

### <a name="thread-execution-states"></a>스레드 실행 상태

스레드의 다양한 처리 상태를 이해하는 것은 전체 다중 스레드 환경을 이해하는 데 중요한 부분입니다. ThreadX에는 준비됨(*ready*), 일시 중단됨(*suspended*), 실행 중(*executing*), 종료됨(*terminated*), 완료됨(*completed*)이라는 다섯 가지 개별 스레드 상태가 있습니다. 그림 5는 ThreadX의 스레드 상태 전환 다이어그램을 보여줍니다.

![스레드 상태 전환](./media/user-guide/thread-state-transition.png)

**그림 5. 스레드 상태 전환**

스레드를 실행할 준비가 되면 준비됨(*ready*) 상태입니다. 준비된 스레드는 준비됨 상태에서 우선 순위가 가장 높은 스레드가 될 때까지 실행되지 않습니다. 그렇게 되면 ThreadX는 스레드를 실행한 다음, 스레드의 상태를 실행 중(*executing*)으로 변경합니다.

우선 순위가 더 높은 스레드가 준비되면 실행 중인 스레드는 다시 준비됨(*ready*) 상태로 돌아갑니다. 그러면 새로 준비된 우선 순위가 높은 스레드가 실행되고 이 스레드의 논리적 상태는 실행 중(*executing*)으로 변경됩니다. 스레드 선점이 발생할 때마다 이렇게 준비됨(*ready*) 상태와 실행 중(*executing*) 상태가 서로 전환되는 경우가 발생합니다.

언제든 특정 순간에 오직 하나의 스레드만 실행 중(*executing*) 상태입니다. 실행 중(*executing*)인 상태의 스레드가 기본 프로세서에 대한 제어권을 갖기 때문입니다.

일시 중단됨(*suspended*) 상태인 스레드는 실행하기에 적합하지 않습니다. 일시 중단됨(*suspended*) 상태인 이유에는 시간, 큐 메시지, 세마포, 뮤텍스, 이벤트 플래그, 메모리에 대한 일시 중단 및 기본 스레드 일시 중단이 포함됩니다. 일시 중단의 원인이 제거되면 스레드는 다시 준비됨(*ready*) 상태로 돌아갑니다.

완료됨(*completed*) 상태인 스레드는 처리가 완료되고 엔트리 함수에서 반환된 스레드입니다. 엔트리 함수는 스레드를 만드는 동안 지정됩니다. 완료됨(*completed*) 상태인 스레드는 다시 실행할 수 없습니다.

스레드가 종료됨(*terminated*) 상태인 이유는 다른 스레드 또는 스레드 자체가 *tx_thread_terminate* 서비스를 호출했기 때문입니다. 종료됨(*terminated*) 상태인 스레드는 다시 실행할 수 없습니다.

> [!IMPORTANT]
> *완료되었거나 종료된 스레드를 다시 시작하는 경우 애플리케이션은 먼저 스레드를 삭제해야 합니다. 그런 다음 다시 만들고 다시 시작할 수 있습니다.*

### <a name="thread-entryexit-notification"></a>스레드 진입/종료 알림

일부 애플리케이션은 특정 스레드에 처음 진입하거나, 스레드가 완료되거나 종료될 때 알림을 받는 것이 유리할 수 있습니다. ThreadX는 ***tx_thread_entry_exit_notify*** 서비스를 통해 이 기능을 제공합니다. 이 서비스는 특정 스레드에 대한 애플리케이션 알림 함수(스레드 실행이 시작되거나, 완료되거나 종료될 때마다 ThreadX에 의해 호출됨)를 등록합니다. 애플리케이션 알림 함수가 호출된 후에는 애플리케이션별 처리를 수행할 수 있습니다. 여기에는 일반적으로 ThreadX 동기화 기본 형식을 통해 다른 애플리케이션 스레드에 이벤트를 알리는 작업이 포함됩니다.

### <a name="thread-priorities"></a>스레드 우선 순위

앞서 언급했듯이 스레드는 전용 용도가 있는 반독립적인 프로그램 세그먼트입니다. 하지만 모든 스레드가 동일하게 생성되는 것은 아닙니다! 일부 스레드의 전용 용도는 다른 스레드보다 훨씬 더 중요합니다. 이렇게 다른 형식의 스레드 중요도는 내장된 실시간 애플리케이션의 특징입니다.

ThreadX는 스레드의 우선 순위(*priority*)를 나타내는 숫자 값을 할당하여 스레드가 생성될 때 스레드의 중요도를 결정합니다. ThreadX 우선 순위의 최대 수는 32에서 1024까지 32씩 증가하도록 구성할 수 있습니다. 실제 최대 우선 순위 수는 ThreadX 라이브러리를 컴파일하는 동안 **TX_MAX_PRIORITIES** 상수에 의해 결정됩니다. 우선 순위의 수가 더 많다고 해서 처리 오버헤드가 크게 증가하지는 않습니다. 하지만 우선 순위 수준이 32개인 각 그룹을 관리하려면 RAM 128바이트가 추가로 필요합니다. 예를 들어, 32개의 우선 순위 수준에는 128바이트의 RAM이 필요하고, 64개의 우선 순위 수준에는 256바이트의 RAM이 필요하며, 96개의 우선 순위 수준에는 384바이트의 RAM이 필요합니다.

기본적으로 ThreadX에는 우선 순위 0부터 우선 순위 31까지 32개의 우선 순위 수준이 있습니다. 숫자 값이 작을수록 높은 우선 순위를 의미합니다. 따라서 우선 순위가 0이면 가장 높은 우선 순위를 나타내고 우선 순위 (**TX_MAX_PRIORITIES**-1)은 가장 낮은 우선 순위를 나타냅니다.

협력 예약 또는 시간 조각화에 따라 여러 스레드가 동일한 우선 순위를 가질 수 있습니다. 또한 스레드 우선 순위는 런타임 중에 변경할 수 있습니다.

### <a name="thread-scheduling"></a>스레드 예약

ThreadX는 우선 순위에 따라 스레드를 예약합니다. 우선 순위가 가장 높은 준비된 스레드가 먼저 실행됩니다. 우선 순위가 같은 여러 스레드가 준비되면 *FIFO(선입 선출)* 방식으로 실행됩니다.

### <a name="round-robin-scheduling"></a>라운드 로빈 예약

ThreadX는 우선 순위가 같은 여러 스레드의 *라운드 로빈* 예약을 지원합니다. 이것은 ***tx_thread_relinquish** _에 대한 협력 호출을 통해 수행됩니다. 이 서비스는 _ *_tx_thread_relinquish_** 호출자가 다시 실행되기 전에 우선 순위가 같은 다른 모든 준비된 스레드가 실행될 수 있는 기회를 제공합니다.

### <a name="time-slicing"></a>시간 조각화

시간 조각화(*Time-slicing*)는 라운드 로빈 예약의 또 다른 형태입니다. 시간 조각은 스레드가 프로세서를 포기하지 않고 실행될 수 있는 최대 타이머 틱(타이머 인터럽트) 수를 지정합니다. ThreadX에서 시간 조각화는 스레드별로 사용할 수 있습니다. 스레드의 시간 조각은 생성 중에 할당되고 런타임 중에 수정할 수 있습니다. 시간 조각이 만료되면 시간이 조각화된 스레드가 다시 실행되기 전에 우선 순위 수준이 같은 다른 모든 준비된 스레드를 실행할 기회가 주어집니다.

일시 중단되거나, 포기되거나, 선점을 유발하는 ThreadX 서비스 호출이 수행되거나 자체적으로 시간이 조각화된 후에 새로운 스레드 시간 조각이 스레드에 제공됩니다.

시간이 조각화된 스레드가 선점되면 나머지 시간 조각에 대해 우선 순위가 동일한 다른 준비된 스레드보다 먼저 재개됩니다.

> [!NOTE]
> *시간 조각화를 사용하면 약간의 시스템 오버헤드가 발생합니다. 시간 조각화는 여러 스레드가 동일한 우선 순위를 공유하는 경우에만 유용하므로 고유한 우선 순위가 있는 스레드에는 시간 조각이 할당되면 안 됩니다.*

### <a name="preemption"></a>선점

선점은 우선 순위가 더 높은 스레드를 위해 실행 중인 스레드를 일시적으로 중단하는 프로세스입니다. 이 프로세스는 실행 중인 스레드에 표시되지 않습니다. 우선 순위가 더 높은 스레드가 완료되면 선점이 발생한 정확한 위치로 제어권이 다시 이전됩니다. 이것은 실시간 시스템에서 매우 중요한 기능입니다. 중요한 애플리케이션 이벤트에 신속하게 대응할 수 있기 때문입니다. 선점은 매우 중요한 기능이지만 고갈, 과도한 오버헤드, 우선 순위 반전을 비롯한 다양한 문제의 원인이 될 수도 있습니다.

### <a name="preemption-thresholdtrade"></a>선점 임계값&trade;

선점에 내재된 문제를 완화하기 위해 ThreadX는 선점 임계값(*preemption-threshold*)이라는 고유한 고급 기능을 제공합니다.

선점 임계값을 사용하면 스레드에서 선점을 비활성화하기 위한 우선 순위 상한(*ceiling*)을 지정할 수 있습니다. 상한보다 우선 순위가 높은 스레드는 여전히 선점이 가능하지만 상한보다 낮은 스레드는 선점이 불가능합니다.

예를 들어 우선 순위가 20인 스레드가 우선 순위가 15에서 20 사이인 스레드 그룹과만 상호 작용한다고 가정합니다. 임계 영역 중에 우선 순위가 20인 스레드는 선점 임계값을 15로 설정하여 상호 작용하는 모든 스레드에서 선점을 방지할 수 있습니다. 이렇게 해도 매우 중요한 스레드(우선 순위가 0에서 14 사이)는 중요한 섹션 처리 중에 이 스레드를 선점할 수 있으며 따라서 응답성이 훨씬 더 향상된 처리가 가능합니다.

물론 선점 임계값을 0으로 설정하여 스레드가 모든 선점을 비활성화하는 것도 가능합니다. 런타임 중에 선점 임계값을 변경할 수도 있습니다.

> [!NOTE]
> *선점 임계값을 사용하면 지정된 스레드에 대한 시간 조각화가 비활성화됩니다.*

### <a name="priority-inheritance"></a>우선 순위 상속

ThreadX는 뮤텍스(이 챕터의 뒷부분에 설명되어 있음) 서비스 내에서 선택적 우선 순위 상속도 지원합니다. 우선 순위 상속을 사용하면 우선 순위가 낮은 스레드가 소유한 뮤텍스를 기다리는 우선 순위가 높은 스레드의 우선 순위를 우선 순위가 낮은 스레드가 임시로 가정할 수 있습니다. 이 기능은 중간 스레드 우선 순위의 선점을 제거하여 애플리케이션이 비결정적 우선 순위 반전을 방지하는 데 유용합니다. 물론, *선점 임계값* 을 사용하여 유사한 결과를 얻을 수 있습니다.

### <a name="thread-creation"></a>스레드 만들기

애플리케이션 스레드는 초기화 중에 또는 다른 애플리케이션 스레드를 실행하는 동안 만들어집니다. 애플리케이션에서 만들 수 있는 스레드 수에는 제한이 없습니다.

### <a name="thread-control-block-tx_thread"></a>스레드 제어 블록 TX_THREAD

각 스레드의 특징은 해당 제어 블록에 포함되어 있습니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

스레드의 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

제어 블록을 다른 영역에서 두려면 모든 동적 할당 메모리와 마찬가지로 조금 더 주의가 필요합니다. 제어 블록이 C 함수 내에 할당된 경우 이것과 연결된 메모리는 호출 스레드 스택의 일부입니다. 일반적으로 제어 블록에 로컬 스토리지를 사용하지 마십시오. 함수가 반환된 후에는 다른 스레드가 로컬 스토리지를 제어 블록에 사용하고 있는지 여부에 관계없이 모든 로컬 변수 스택 공간이 해제되기 때문입니다.

대부분의 경우 애플리케이션은 스레드 제어 블록의 콘텐츠를 인식하지 못합니다. 하지만 특정 멤버를 확인하는 것이 유용한 상황이 있습니다(특히 디버그 중에). 다음은 몇 가지 유용한 제어 블록 멤버 중 일부입니다.

**tx_thread_run_count** 에는 스레드가 예약된 횟수 카운터가 포함됩니다. 카운터가 증가하면 스레드가 예약되고 실행되고 있음을 나타냅니다.

**tx_thread_state** 에는 연결된 스레드의 상태가 포함됩니다. 가능한 스레드 상태는 다음과 같습니다.

|  스레드 상태   | 값 |
| --------------- | ------ |
| TX_READY       | (0x00) |
| TX_COMPLETED   | (0x01) |
| TX_TERMINATED  | (0x02) |
| TX_SUSPENDED   | (0x03) |
| TX_SLEEP       | (0x04) |
| TX_QUEUE_SUSP | (0x05) |
| TX_SEMAPHORE_SUSP | (0x06) |
| TX_EVENT_FLAG   | (0x07) |
| TX_BLOCK_MEMORY | (0x08) |
| TX_BYTE_MEMORY  | (0x09) |
| TX_MUTEX_SUSP   | (0x0D) |

> [!NOTE]
> *물론, 스레드 제어 블록에는 다른 흥미로운 필드(스택 포인터, 시간 조각 값, 우선 순위 등)가 많이 있습니다. 사용자는 제어 블록 멤버를 검토할 수 있지만 수정은 엄격히 금지됩니다!*

> [!IMPORTANT]
> *이는 이 섹션 앞부분에서 언급한 "실행 중" 상태와는 다릅니다. 주어진 시간에 실행되는 스레드가 하나뿐이기 때문에 실행 중 상태는 필요하지 않습니다. 또한 실행 중인 스레드의 상태도* **TX_READY** 입니다.

### <a name="currently-executing-thread"></a>현재 실행 중인 스레드

전에 언급했듯이 주어진 시간에 스레드는 하나만 실행됩니다. 실행 중인 스레드를 식별하는 방법은 요청을 수행하는 스레드에 따라 여러 가지가 있습니다.
프로그램 세그먼트는 ***tx_thread_identify*** 를 호출하여 실행 중인 스레드의 제어 블록 주소를 가져올 수 있습니다. 이것은 여러 스레드에서 실행되는 애플리케이션 코드의 공유 부분에서 유용합니다.

디버그 세션에서 사용자는 내부 ThreadX 포인터인 ***_tx_thread_current_ptr*** 을 검사할 수 있습니다. 여기에는 현재 실행 중인 스레드의 제어 블록 주소가 포함됩니다. 이 포인터가 NULL이면 애플리케이션 스레드는 실행되지 않습니다. 즉, ThreadX는 스레드가 준비될 때까지 예약 루프에서 대기하고 있습니다.

### <a name="thread-stack-area"></a>스레드 스택 영역

각 스레드에는 마지막 실행 및 컴파일러 사용의 컨텍스트를 저장하기 위한 자체 스택이 있어야 합니다. 대부분의 C 컴파일러는 이 스택을 사용하여 함수 호출을 수행하고 임시로 로컬 변수를 할당합니다. 그림 6은 일반적인 스레드 스택을 보여줍니다.

메모리에서 스레드 스택이 어디에 위치할지는 애플리케이션에 달려 있습니다. 스택 영역은 스레드 생성 중에 지정되며 대상 주소 공간의 어디에나 위치할 수 있습니다. 이 기능을 사용하면 애플리케이션이 고속 RAM에 스택을 배치하여 중요한 스레드의 성능을 향상시킬 수 있기 때문에 중요합니다.

**스택 메모리 영역**(예)

![일반적인 스레드 스택](./media/user-guide/typical-thread-stack.png)

**그림 6. 일반적인 스레드 스택**

스택에 필요한 크기는 스레드에 대해 가장 자주 묻는 질문 중 하나입니다. 스레드의 스택 영역은 최악의 경우 함수 호출 중첩, 로컬 변수 할당 및 마지막 실행 컨텍스트 저장을 수용할 수 있을 만큼 충분히 커야 합니다.

최소 스택 크기 **TX_MINIMUM_STACK** 은 ThreadX에 의해 정의됩니다. 이 크기의 스택은 스레드의 컨텍스트 저장 및 최소한의 함수 호출 그리고 로컬 변수 할당을 지원합니다.

하지만 대부분의 스레드는 최소 스택 크기가 너무 작기 때문에 사용자가 함수 호출 중첩 및 로컬 변수 할당을 검사하여 최악의 경우 크기 요구 사항을 확인해야 합니다. 물론, 시작하는 스택 영역이 클수록 좋습니다.

애플리케이션을 디버깅한 후 메모리가 부족하면 스레드 스택 크기를 조정할 수 있습니다. 자주 사용하는 트릭은 스레드를 만들기 전에 쉽게 식별할 수 있는 (0xEFEF)와 같은 데이터 패턴으로 모든 스택 영역을 미리 설정하는 것입니다. 애플리케이션이 얼마나 잘 작동하는지 철저히 시험해 본 후 스택 영역을 검사하여 데이터 패턴이 아직 온전한 스택 영역을 찾아서 실제로 얼마나 많은 스택이 사용되었는지 확인할 수 있습니다. 그림 7은 철저한 스레드 실행 후 0xEFEF로 사전 설정된 스택을 보여줍니다.

**스택 메모리 영역**(다른 예)

![0xEFEF로 스택 사전 설정*](./media/user-guide/stack-preset.png)

**그림 7. 0xEFEF로 스택 사전 설정**

> [!IMPORTANT]
> *기본적으로 ThreadX는 각 스레드 스택의 모든 바이트를 0xEF 값으로 초기화합니다.*

### <a name="memory-pitfalls"></a>메모리 관련 문제

스레드에 대한 스택 요구 사항은 클 수 있습니다. 따라서, 애플리케이션이 적절한 수의 스레드를 갖도록 디자인하는 것이 중요합니다. 또한 스레드 내에서 과도한 스택 사용을 방지하기 위해 약간의 주의를 기울여야 합니다. 재귀 알고리즘과 대규모 로컬 데이터 구조는 피해야 합니다.

대부분의 경우 스택이 오버플로되면 스레드 실행으로 인해 스택 영역에 인접한(일반적으로 이전) 메모리가 손상됩니다. 결과는 예측할 수 없지만 대부분의 경우 프로그램 카운터가 부자연스럽게 변경됩니다. 이를 흔히 "잡초에 뛰어들기(jumping into the weeds)"라고 합니다. 물론 이것을 방지하는 유일한 방법은 모든 스레드 스택이 충분히 큰지 확인하는 것입니다.

### <a name="optional-run-time-stack-checking"></a>선택적 런타임 스택 검사

ThreadX는 런타임 중에 각 스레드의 스택이 손상되었는지 확인하는 기능을 제공합니다. 기본적으로 ThreadX는 생성하는 동안 스레드 스택의 모든 바이트를 0xEF 데이터 패턴으로 채웁니다. 애플리케이션이 **TX_ENABLE_STACK_CHECKING** 이 정의된 ThreadX 라이브러리를 빌드하는 경우 ThreadX는 일시 중단되거나 재개될 때 각 스레드의 스택이 손상되었는지 검사합니다. 스택 손상이 감지되면 **_tx_thread_stack_error_notify_ *_ 호출에 지정된 대로 애플리케이션의 스택 오류 처리 루틴을 호출합니다. 스택 오류 처리기가 지정되지 않은 경우 ThreadX는 내부 _* _ _tx_thread_stack_error_handler_** 루틴을 호출합니다.

### <a name="reentrancy"></a>다시 표시

다중 스레딩의 정말 좋은 점 중 하나는 동일한 C 함수를 여러 스레드에서 호출할 수 있다는 것입니다. 이를 통해 강력한 성능을 제공하고 코드 공간을 줄일 수 있습니다. 하지만 다중 스레드에서 호출된 C 함수가 재진입(*reentrant*) 함수여야 합니다.

기본적으로 재진입 함수는 호출자의 반환 주소를 현재 스택에 저장하고 이전에 설정한 전역 또는 정적 C 변수에 의존하지 않습니다. 대부분의 컴파일러는 스택에 반환 주소를 배치합니다. 따라서 애플리케이션 개발자는 전역(*globals*) 및 정적(*statics*) 사용에 대해서만 고려해야 합니다.

재진입이 아닌 함수의 예는 표준 C 라이브러리에 있는 문자열 토큰 함수 ***strtok*** 입니다. 이 함수는 후속 호출에서 이전 문자열 포인터를 "기억"합니다. 이 작업은 정적 문자열 포인터를 사용하여 수행합니다. 이 함수를 여러 스레드에서 호출하면 잘못된 포인터를 반환할 가능성이 높습니다.

### <a name="thread-priority-pitfalls"></a>스레드 우선 순위 관련 문제

스레드 우선 순위 선택은 다중 스레딩의 가장 중요한 측면 중 하나입니다. 런타임에 정확히 무엇이 필요한지 결정하기 보다는 스레드 중요도에 대해 인식된 개념을 기반으로 우선 순위를 할당하려고 하는 경우가 많습니다. 스레드 우선 순위를 잘못 사용하면 다른 스레드가 고갈되고 우선 순위 반전이 생성되며 처리 대역폭이 줄어들고 애플리케이션의 런타임 동작을 이해하기 어려울 수 있습니다.

앞서 언급했듯이 ThreadX는 우선 순위에 기반한 선점형 예약 알고리즘을 제공합니다. 우선 순위가 낮은 스레드는 실행할 준비가 된 우선 순위가 더 높은 스레드가 없을 때까지 실행되지 않습니다. 우선 순위가 더 높은 스레드가 항상 준비되어 있으면 우선 순위가 낮은 스레드는 실행되지 않습니다. 이러한 상황을 스레드 고갈(*thread starvation*)이라고 합니다.

대부분의 스레드 고갈 문제는 디버그 초기에 감지되며 우선 순위가 더 높은 스레드가 지속적으로 실행되지 않도록 하여 문제를 해결할 수 있습니다. 또는 고갈된 스레드가 실행 기회를 얻을 때까지 우선 순위를 점진적으로 올리는 논리를 애플리케이션에 추가할 수도 있습니다.

스레드 우선 순위와 관련된 또 다른 문제는 우선 순위 반전(*priority inversion*)입니다. 우선 순위 반전은 우선 순위가 낮은 스레드에 필요한 리소스가 있기 때문에 우선 순위가 높은 스레드가 일시 중단되는 경우 발생합니다. 물론, 우선 순위가 서로 다른 두 스레드가 공용 리소스를 공유해야 하는 경우도 있습니다. 이 스레드가 유일한 활성 스레드인 경우 우선 순위가 낮은 스레드가 리소스를 보유하는 시간에 의해 우선 순위 반전 시간이 제한됩니다. 이 조건은 결정적이며 매우 정상적입니다. 하지만 이 우선 순위 반전 조건 중에 중간 우선 순위의 스레드가 활성 상태가 되면 우선 순위 반전 시간은 더 이상 결정적이지 않고 애플리케이션 오류를 유발할 수 있습니다.

ThreadX에서 비결정적 우선 순위 반전을 방지하는 세 가지 다른 메서드가 있습니다. 첫째, 애플리케이션 우선 순위 선택 및 런타임 동작을 우선 순위 반전 문제를 방지하는 방식으로 설계할 수 있습니다. 둘째, 우선 순위가 낮은 스레드는 *선점 임계값* 을 활용하여 우선 순위가 더 높은 스레드와 리소스를 공유하는 동안 중간 스레드의 선점을 차단할 수 있습니다. 마지막으로 ThreadX 뮤텍스 개체를 사용하여 시스템 리소스를 보호하는 스레드는 선택적 뮤텍스 *우선 순위 상속* 을 활용하여 비결정적 우선 순위 반전을 제거할 수 있습니다.

### <a name="priority-overhead"></a>우선 순위 오버 헤드

다중 스레딩에서 오버헤드를 줄이는 데 가장 간과되는 방법 중 하나는 컨텍스트 전환 수를 줄이는 것입니다. 앞서 언급했듯이 컨텍스트 전환은 실행 중인 스레드보다 우선 순위가 더 높은 스레드 실행을 선호하는 경우에 발생합니다. 우선 순위가 더 높은 스레드는 외부 이벤트(예: 인터럽트) 및 실행 중인 스레드가 만든 서비스 호출의 결과로 준비될 수 있습니다.

스레드 우선 순위가 컨텍스트 전환 오버헤드에 미치는 영향을 설명하기 위해 *thread_1*, *thread_2*, *thread_3* 이라는 스레드로 3개 스레드 환경을 가정하겠습니다. 모든 스레드가 메시지를 기다리는 일시 중단 상태에 있다고 가정합니다. thread_1이 메시지를 받으면 thread_2에 즉시 전달합니다. 그러면 thread_2는 thread_3에 메시지를 전달합니다. thread_3은 메시지를 버립니다. 각 스레드는 메시지를 처리한 후 돌아가서 또 다른 메시지를 기다립니다.

3개 스레드를 실행하는 데 필요한 처리는 우선 순위에 따라 크게 달라집니다. 모든 스레드의 우선 순위가 동일하면 각 스레드를 실행하기 전에 단일 컨텍스트 전환이 발생합니다. 컨텍스트 전환은 각 스레드가 빈 메시지 큐에서 일시 중단될 때 발생합니다.

하지만 thread_2가 thread_1보다 우선 순위가 높고 thread_3이 thread_2보다 우선 순위가 높으면 컨텍스트 전환 수는 두 배가 됩니다. 우선 순위가 더 높은 스레드가 준비된 것을 감지하면 *tx_queue_send* 서비스 내부에서 다른 컨텍스트 전환이 발생하기 때문입니다.

ThreadX 선점 임계값 메커니즘은 이러한 추가 컨텍스트 전환을 방지하면서 앞서 언급한 우선 순위 선택을 허용할 수 있습니다. 이 기능을 사용하면 예약 중에 여러 스레드 우선 순위가 허용되는 동시에 스레드 실행 중에 원치 않는 컨텍스트 전환을 제거할 수 있기 때문에 중요합니다.

### <a name="run-time-thread-performance-information"></a>런타임 스레드 성능 정보

ThreadX는 선택적 런타임 스레드 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_THREAD_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 스레드 재개

  - 스레드 일시 중단

  - 서비스 호출 선점

  - 인터럽트 선점

  - 우선 순위 반전

  - 시간 조각

  - 포기

  - 스레드 시간 제한

  - 일시 중단을 중단

  - 유휴 시스템 반환

  - 비유휴 시스템 반환

각 스레드의 총 수:

  - 재개

  - 일시 중단

  - 서비스 호출 선점

  - 인터럽트 선점

  - 우선 순위 반전

  - 시간 조각

  - 스레드 포기

  - 스레드 시간 제한

  - 일시 중단을 중단

이 정보는 ***tx_thread_performance_info_get** _ 및 _*_tx_thread_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 스레드 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 서비스 호출 선점의 수가 상대적으로 높으면 스레드의 우선 순위 및/또는 선점 임계값이 너무 낮다는 표시일 수 있습니다. 또한 유휴 시스템 반환 수가 상대적으로 적으면 우선 순위가 낮은 스레드가 충분히 일시 중단되지 않음을 의미할 수 있습니다.

### <a name="debugging-pitfalls"></a>디버깅 관련 문제

다중 스레드 애플리케이션을 디버깅하는 것은 동일한 프로그램 코드가 여러 스레드에서 실행될 수 있기 때문에 더 어렵습니다. 이러한 경우 중단점만으로는 부족할 수 있습니다. 디버거는 조건부 중단점을 사용하여 현재 스레드 포인터 **_tx_thread_current_ptr** 도 살펴보고 호출 스레드가 디버깅할 스레드가 맞는지 확인해야 합니다.

이 중 대부분은 다양한 개발 도구 공급업체를 통해 제공되는 다중 스레딩 지원 패키지에서 처리됩니다. 디자인이 단순하기 때문에 ThreadX를 다른 개발 도구와 통합하는 것이 비교적 쉽습니다.

스택 크기는 다중 스레딩에서 항상 중요한 디버그 항목입니다. 설명할 수 없는 동작이 보일 때마다 모든 스레드의 스택 크기(특히 마지막으로 실행할 스레드의 스택 크기)를 늘리는 것이 좋습니다!

> [!TIP]
> ***TX_ENABLE_STACK_CHECKING** 으로 정의된 threadx 라이브러리를 빌드하는 것도 좋은 방법입니다. 이를 통해 가능한 한 처리 초기에 스택 손상 문제를 격리할 수 있습니다.*

## <a name="message-queues"></a>메시지 큐

메시지 큐는 ThreadX에서 스레드 간 통신의 기본 수단입니다. 메시지 큐에는 하나 이상의 메시지가 있을 수 있습니다. 단일 메시지를 보유하는 메시지 큐를 일반적으로 사서함(*mailbox*)이라고 합니다.

메시지는 ***tx_queue_send** _에 의해 큐로 복사되고 _*_tx_queue_receive_**에 의해 큐에서 복사됩니다. 이에 대한 유일한 예외는 빈 큐에서 메시지를 기다리는 동안 스레드가 일시 중단되는 경우입니다. 이 경우 큐에 전송된 다음 메시지는 스레드의 대상 영역에 직접 배치됩니다.

각 메시지 큐는 공용 리소스입니다. ThreadX는 메시지 큐가 사용되는 방식에 제약 조건을 두지 않습니다.

### <a name="creating-message-queues"></a>메시지 큐 만들기

메시지 큐는 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 애플리케이션의 메시지 큐 수에는 제한이 없습니다.

### <a name="message-size"></a>메시지 크기

각 메시지 큐는 여러 가지 고정 크기 메시지를 지원합니다. 사용 가능한 메시지 크기는 32비트 1~16단어입니다. 메시지 크기는 큐를 만들 때 지정됩니다. 16단어 보다 큰 애플리케이션 메시지는 포인터로 전달되어야 합니다. 이 작업은 메시지 크기가 1단어(포인터를 보유하기에 충분함)인 큐를 만든 다음, 전체 메시지 대신 메시지 포인터를 보내고 받는 방식으로 수행됩니다.

### <a name="message-queue-capacity"></a>메시지 큐 용량

큐가 보유할 수 있는 메시지 수는 메시지 크기 및 생성 중에 제공된 메모리 영역의 크기의 함수입니다. 큐의 총 메시지 용량은 각 메시지의 바이트 수를 제공된 메모리 영역의 총 바이트 수로 나누어 계산합니다.

예를 들어, 메시지 크기로 32비트 1단어(4바이트)를 지원하는 메시지 큐가 100바이트 메모리 영역으로 생성되는 경우 용량은 25개 메시지입니다.

### <a name="queue-memory-area"></a>큐 메모리 영역

앞서 언급했듯이 메시지 버퍼링을 위한 메모리 영역은 큐를 만드는 동안 지정됩니다. ThreadX의 다른 메모리 영역과 마찬가지로 대상 주소 공간의 어디에나 위치할 수 있습니다.

애플리케이션에 상당한 유연성을 제공하기 때문에 중요한 기능입니다. 예를 들어 애플리케이션은 성능 향상을 위해 중요한 큐의 메모리 영역을 고속 RAM에 배치할 수 있습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

애플리케이션 스레드는 큐에서 메시지를 받거나 보내는 동안 일시 중단될 수 있습니다. 일반적으로 스레드 일시 중단에는 빈 큐에서 메시지를 기다리는 작업이 포함됩니다. 하지만 꽉 찬 큐에 메시지를 보내려는 시도를 스레드가 일시 중단하는 것도 가능합니다.

일시 중단 조건이 확인되면 요청된 서비스가 완료되고 대기 중인 스레드가 재개됩니다. 여러 스레드가 동일한 큐에서 일시 중단되면 일시 중단된 순서대로(FIFO) 재개됩니다.

하지만 스레드 일시 중단을 해제하는 큐 서비스 이전에 애플리케이션이 ***tx_queue_prioritize*** 를 호출하는 경우에도 우선 순위 재개가 가능합니다. 큐 우선 순위 지정 서비스는 우선 순위가 가장 높은 스레드를 일시 중단 목록의 맨 앞에 배치하고 다른 모든 일시 중단된 스레드는 동일한 FIFO 순서를 유지합니다.

시간 제한은 모든 큐 일시 중단에도 사용할 수 있습니다. 기본적으로 시간 제한은 스레드가 일시 중단된 상태로 유지되는 최대 타이머 틱 수를 지정합니다. 시간 제한이 발생하면 스레드가 재개되고 서비스가 적절한 오류 코드와 함께 반환됩니다.

### <a name="queue-send-notification"></a>큐 알림 보내기

일부 애플리케이션은 메시지가 큐에 배치될 때마다 알림을 받는 것이 유리할 수 있습니다. ThreadX는 ***tx_queue_send_notify*** 서비스를 통해 이 기능을 제공합니다. 이 서비스는 제공된 애플리케이션 알림 함수를 지정된 큐에 등록합니다. ThreadX는 메시지를 큐에 보낼 때마다 이 애플리케이션 알림 함수를 호출합니다. 애플리케이션 알림 함수 내에서 정확한 처리는 애플리케이션에 의해 결정됩니다. 하지만 일반적으로 새 메시지를 처리하기 위한 적절한 스레드를 재개하는 것으로 구성됩니다.

### <a name="queue-event-chainingtrade"></a>Queue Event chaining&trade;

ThreadX의 알림 기능을 사용하여 다양한 동기화 이벤트를 함께 연결할 수 있습니다. 일반적으로 단일 스레드가 여러 동기화 이벤트를 처리해야 하는 경우에 유용 합니다.

예를 들어, 단일 스레드가 5개의 서로 다른 큐에서 메시지 처리를 담당하고 사용 가능한 메시지가 없을 때도 일시 중지해야 한다고 가정합니다. 이 작업은 각 큐에 대한 애플리케이션 알림 함수를 등록하고 추가 카운팅 세마포를 도입하여 쉽게 수행할 수 있습니다. 특히, 애플리케이션 알림 함수는 호출될 때마다 *tx_semaphore_put* 을 수행합니다. (세마포 수는 5개 큐 모두의 총 메시지 수를 나타냅니다.) 처리 스레드는 *tx_semaphore_get* 서비스를 통해 이 세마포에서 일시 중단됩니다. 세마포를 사용할 수 있으면(여기서는 메시지를 사용할 수 있는 경우) 처리 스레드가 재개됩니다. 그런 다음, 각 큐에 메시지가 있는지 조사하여, 발견된 메시지를 처리하고, 또 다른 ***tx_semaphore_get*** 을 수행하여 다음 메시지를 기다립니다. 이것을 이벤트 연결 없이 구현하려면 매우 어렵고 더 많은 스레드 및/또는 추가 애플리케이션 코드가 필요할 수 있습니다.

일반적으로 이벤트 연결(*event-chaining*)은 스레드 수, 오버헤드, RAM 요구 사항을 줄입니다. 또한 복잡한 시스템의 동기화 요구 사항을 처리할 수 있는 매우 유연한 메커니즘을 제공합니다.

### <a name="run-time-queue-performance-information"></a>런타임 큐 성능 정보
ThreadX는 선택적 런타임 큐 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 ***TX_QUEUE_ENABLE_PERFORMANCE_INFO*** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 전송된 메시지

  - 수신된 메시지

  - 큐 비어 있음 일시 중단

  - 큐 가득 참 일시 중단

  - 큐 가득 참 오류 반환(일시 중단이 지정되지 않음)

  - 큐 시간 제한

각 큐의 다음에 대한 총 수:

  - 전송된 메시지

  - 수신된 메시지

  - 큐 비어 있음 일시 중단

  - 큐 가득 참 일시 중단

  - 큐 가득 참 오류 반환(일시 중단이 지정되지 않음)

  - 큐 시간 제한

이 정보는 ***tx_queue_performance_info_get** _ 및 _*_tx_queue_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 큐 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 "큐 가득 참 일시 중단"의 수가 상대적으로 높으면 큐 크기를 늘리는 것이 유용하다는 표시일 수 있습니다.

### <a name="queue-control-block-tx_queue"></a>큐 제어 블록 TX_QUEUE

각 메시지 큐의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 큐에 있는 메시지의 수와 같은 흥미로운 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

메시지 큐 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="message-destination-pitfall"></a>메시지 대상 관련 문제

앞서 언급했듯이 메시지는 큐 영역과 애플리케이션 데이터 영역 간에 복사됩니다. 받은 메시지의 대상이 전체 메시지를 보관할 수 있을 정도로 큰지 확인하는 것이 중요합니다. 그렇지 않으면 메시지 대상 다음의 메모리가 손상될 수 있습니다.

> [!NOTE]
> *스택에 너무 작은 메시지 대상이 있는 경우 특히 치명적이며, 함수의 반환 주소를 손상시키는 것과는 전혀 다릅니다!*

## <a name="counting-semaphores"></a>세마포 수 계산

ThreadX는 32비트 카운팅 세마포를 제공하며 이 값의 범위는 0에서 4,294,967,295 사이입니다. 카운팅 세마포에는 *tx_semaphore_get* 및 *tx_semaphore_put* 이라는 두 가지 연산이 있습니다. get 연산은 세마포를 하나씩 줄입니다. 세마포가 0이면 get 연산이 실패합니다. get 연산의 반대는 put 연산입니다.
세마포를 하나씩 증가시킵니다.

각 카운팅 세마포는 공용 리소스입니다. ThreadX는 카운팅 세마포가 사용되는 방식에 대한 제약 조건을 두지 않습니다.

카운팅 세마포는 일반적으로 상호 배제(*mutual exclusion*)에 사용됩니다. 하지만 카운팅 세마포는 이벤트 알림을 위한 메서드로도 사용될 수 있습니다.

### <a name="mutual-exclusion"></a>상호 배제

 상호 배제는 특정 애플리케이션 영역(*임계 영역* 또는 *애플리케이션 리소스* 라고도 함)에 대한 스레드 액세스 제어와 관련이 있습니다. 상호 배제에 사용되는 경우 세마포의 "현재 개수"는 액세스가 허용된 총 스레드 수를 나타냅니다. 대부분의 경우 상호 배제에 사용되는 카운팅 세마포의 초기 값은 1이며, 이것은 연결된 리소스에 스레드가 한 번에 하나만 액세스할 수 있다는 의미입니다. 값이 0 또는 1인 카운팅 세마포를 일반적으로 이진 세마포(*binary semaphores*)라고 합니다.

> [!IMPORTANT]
> *이진 세마포가 사용되는 경우 사용자는 이미 소유하고 있는 세마포에 대해 동일한 스레드가 get 작업을 수행할 수 없도록 해야 합니다. 두 번째 get은 실패하고 호출 스레드를 무기한으로 일시 중단하고 리소스를 영구적으로 사용할 수 없도록 할 수 있습니다.*

### <a name="event-notification"></a>이벤트 알림

카운팅 세마포를 생산자-소비자 방식의 이벤트 알림으로 사용할 수도 있습니다. 소비자는 카운팅 세마포를 가져오려고 시도하는 반면 생산자는 사용 가능한 항목이 있을 때마다 세마포를 증가시킵니다. 이러한 세마포는 일반적으로 초기 값이 0이며 생산자가 소비자에 대해 준비될 때까지 증가하지 않습니다. 이벤트 알림에 사용되는 세마포가 ***tx_semaphore_ceiling_put*** 서비스 호출을 사용하면 유용할 수 있습니다. 이 서비스는 세마포 수가 호출에 제공된 값을 절대 초과하지 않도록 합니다.

### <a name="creating-counting-semaphores"></a>카운팅 세마포 만들기

카운팅 세마포는 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 세마포의 초기 개수는 생성 중에 지정됩니다. 애플리케이션의 카운팅 세마포 수에는 제한이 없습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

현재 개수가 0인 세마포에서 get 연산을 수행하는 동안 애플리케이션 스레드가 일시 중단될 수 있습니다.

put 연산이 수행된 후 일시 중단된 스레드의 get 연산이 수행되고 스레드가 재개됩니다. 동일한 카운팅 세마포에서 여러 스레드가 일시 중단되면 일시 중단된 순서 대로(FIFO) 재개됩니다.

하지만 스레드 일시 중단을 해제하는 세마포 put 호출 전에 애플리케이션이 ***tx_semaphore_prioritize*** 를 호출하는 경우에도 우선 순위 재개가 가능합니다. 세마포 우선 순위 지정 서비스는 우선 순위가 가장 높은 스레드를 일시 중단 목록의 맨 앞에 배치하고 다른 모든 일시 중단된 스레드는 동일한 FIFO 순서를 유지합니다.

### <a name="semaphore-put-notification"></a>세마포 Put 알림

일부 애플리케이션은 세마포를 put할 때마다 알림을 받는 것이 유리할 수 있습니다. ThreadX는 ***tx_semaphore_put_notify*** 서비스를 통해 이 기능을 제공합니다. 이 서비스는 제공된 애플리케이션 알림 함수를 지정된 세마포에 등록합니다. ThreadX는 이후에 세마포를 put할 때마다 이 애플리케이션 알림 함수를 호출합니다. 애플리케이션 알림 함수 내에서 정확한 처리는 애플리케이션에 의해 결정됩니다. 하지만 일반적으로 새 세마포 put 이벤트를 처리하기 위한 적절한 스레드를 재개하는 것으로 구성됩니다.

### <a name="semaphore-event-chainingtrade"></a>Semaphore Event chaining&trade;

ThreadX의 알림 기능을 사용하여 다양한 동기화 이벤트를 함께 연결할 수 있습니다. 일반적으로 단일 스레드가 여러 동기화 이벤트를 처리해야 하는 경우에 유용 합니다.

예를 들어 큐 메시지, 이벤트 플래그, 세마포에 대한 개별 스레드를 일시 중단하는 대신 애플리케이션은 각 개체에 대한 알림 루틴을 등록할 수 있습니다. 호출되면 애플리케이션 알림 루틴은 단일 스레드를 재개할 수 있습니다. 그러면 각 개체를 조사하여 새 이벤트를 찾아서 처리할 수 있습니다.

일반적으로 이벤트 연결(*event-chaining*)은 스레드 수, 오버헤드, RAM 요구 사항을 줄입니다. 또한 복잡한 시스템의 동기화 요구 사항을 처리할 수 있는 매우 유연한 메커니즘을 제공합니다.

### <a name="run-time-semaphore-performance-information"></a>런타임 세마포 성능 정보

ThreadX는 선택적 런타임 세마포 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_SEMAPHORE_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 세마포 put

  - 세마포 get

  - 세마포 get 일시 중단

  - 세마포 get 시간 제한

각 세마포의 다음에 대한 총 수:

  - 세마포 put

  - 세마포 get

  - 세마포 get 일시 중단

  - 세마포 get 시간 제한

이 정보는 ***tx_semaphore_performance_info_get** _ 및 _*_tx_semaphore_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 세마포 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 "세마포 get 시간 제한"의 수가 상대적으로 높으면 다른 스레드가 리소스를 너무 오래 보유하고 있다는 표시일 수 있습니다.

### <a name="semaphore-control-block-tx_semaphore"></a>세마포 제어 블록 TX_SEMAPHORE

각 카운팅 세마포의 특징은 해당 제어 블록에서 찾을 수 있습니다. 현재 세마포 수와 같은 정보를 포함합니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

세마포 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="deadly-embrace"></a>치명적인 수용

상호 배제에 사용되는 세마포와 관련하여 가장 흥미롭고 위험한 문제 중 하나는 치명적인 수용(*deadly embrace*)입니다. 치명적인 수용 또는 *교착* 상태는 이미 서로를 소유하고 있는 세마포를 get하려고 시도하는 동안 둘 이상의 스레드가 무기한 일시 중단되는 조건입니다.

이 조건은 두 개의 스레드, 두 개의 세마포 예제로 가장 잘 설명됩니다. 첫 번째 스레드가 첫 번째 세마포를 소유하고 두 번째 스레드가 두 번째 세마포를 소유한다고 가정합니다. 첫 번째 스레드가 두 번째 세마포를 get하려고 시도하는 동시에 두 번째 스레드가 첫 번째 세마포를 get하려고 시도하면 두 스레드 모두 교착 상태가 됩니다. 또한 이 스레드가 일시 중단된 상태로 영원히 유지되면 연결된 리소스 역시 영원히 잠깁니다. 그림 8에서는 이 예를 보여 줍니다.

**치명적인 수용**(예)

![일시 중단된 스레드의 예](./media/user-guide/example-suspended-threads.png)

**그림 8. 일시 중단된 스레드의 예**

실시간 시스템의 경우 스레드가 세마포를 확보하는 방법에 특정 제한을 설정하여 치명적인 수용을 방지할 수 있습니다. 스레드는 세마포를 한 번에 하나만 가질 수 있습니다. 또는, 스레드와 세마포가 동일한 순서로 모이면 여러 스레드가 여러 세마포를 소유할 수 있습니다. 앞의 예에서 첫 번째와 두 번째 스레드가 첫 번째와 두 번째 세마포를 순서대로 확보하면 치명적인 수용이 방지됩니다.

> [!TIP]
> *get 연산과 관련된 중단 시간 제한을 사용하여 치명적인 수용에서 복구할 수도 있습니다.*

### <a name="priority-inversion"></a>우선 순위 반전

상호 배제 세마포와 관련된 또 다른 문제는 우선 순위 반전입니다. 이 항목은 "[스레드 우선 순위 관련 문제](#thread-priority-pitfalls)"에서 자세히 설명되어 있습니다.

기본적인 문제는 우선 순위가 높은 스레드에 필요한 세마포가 우선 순위가 낮은 스레드에 있는 상황에서 발생합니다. 이것 자체로는 정상입니다. 하지만 스레드의 우선 순위가 그 사이이면 우선 순위 반전이 비결정적 시간 동안 지속될 수 있습니다. 이런 문제는 스레드 우선 순위를 신중하게 선택하고, 선점 임계값을 사용하고, 리소스를 소유한 스레드의 우선 순위를 우선 순위가 높은 스레드의 우선 순위로 일시적으로 높여서 처리할 수 있습니다.

## <a name="mutexes"></a>뮤텍스

ThreadX는 세마포 외에 뮤텍스 개체도 제공합니다. 뮤텍스는 기본적으로 이진 세마포입니다. 즉, 한 번에 하나의 스레드만 뮤텍스를 소유할 수 있습니다. 또한 동일한 스레드는 소유한 뮤텍스에서 성공적인 뮤텍스 get 연산을 여러 번(정확히 4,294,967,295번) 수행할 수 있습니다. 뮤텍스 개체에는 ***tx_mutex_get** _ 및 _*_tx_mutex_put_**이라는 두 가지 연산이 있습니다. get 연산은 다른 스레드가 소유하지 않은 뮤텍스를 확보하는 반면, put 연산은 이전에 확보한 뮤텍스를 해제합니다. 스레드가 뮤텍스를 해제하려면 put 연산의 수가 이전 get 연산의 수와 같아야 합니다.

각 뮤텍스는 공용 리소스입니다. ThreadX는 뮤텍스가 사용되는 방식에 제약 조건을 두지 않습니다.

ThreadX 뮤텍스는 *상호 배제* 에만 사용됩니다. 카운팅 세마포와 달리, 뮤텍스는 이벤트 알림을 위한 메서드로 사용되지 않습니다.

### <a name="mutex-mutual-exclusion"></a>뮤텍스 상호 배제

카운팅 세마포 섹션의 설명과 마찬가지로, 상호 배제는 특정 애플리케이션 영역(*임계 영역* 또는 *애플리케이션 리소스* 라고도 함)에 대한 스레드 액세스 제어와 관련이 있습니다. 사용 가능한 경우 ThreadX 뮤텍스의 소유권 개수는 0입니다. 스레드가 뮤텍스를 확보한 후 소유권 개수는 뮤텍스에서 수행한 get 연산이 성공할 때마다 한 번씩 증가하고 put 연산이 성공할 때마다 한 번씩 감소합니다.

### <a name="creating-mutexes"></a>뮤텍스 만들기

ThreadX 뮤텍스는 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 뮤텍스의 초기 조건은 항상 "사용 가능"입니다. *우선 순위 상속* 을 선택한 상태에서 뮤텍스를 만들 수도 있습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

다른 스레드가 이미 소유하고 있는 뮤텍스에 대해 get 연산을 수행하는 동안 애플리케이션 스레드가 일시 중단될 수 있습니다.

소유하는 스레드가 동일한 수의 put 연산을 수행한 후 일시 중단된 스레드의 get 연산이 수행되어 뮤텍스의 소유권을 부여하고 스레드가 재개됩니다. 여러 스레드가 동일한 뮤텍스에서 일시 중단되면 일시 중단된 순서대로(FIFO) 재개됩니다.

단, 생성 중에 뮤텍스 우선 순위 상속을 선택했으면 우선 순위 재개가 자동으로 수행됩니다. 스레드 일시 중단을 해제하는 뮤텍스 put 호출 전에 애플리케이션이 ***tx_mutex_prioritize*** 를 호출하는 경우에도 우선 순위 재개가 가능합니다. 뮤텍스 우선 순위 지정 서비스는 우선 순위가 가장 높은 스레드를 일시 중단 목록의 맨 앞에 배치하고 다른 모든 일시 중단된 스레드는 동일한 FIFO 순서를 유지합니다.

### <a name="run-time-mutex-performance-information"></a>런타임 뮤텍스 성능 정보

ThreadX는 선택적 런타임 뮤텍스 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_MUTEX_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

- 뮤텍스 put

- 뮤텍스 get

- 뮤텍스 get 일시 중단

- 뮤텍스 get 시간 제한

- 뮤텍스 우선 순위 반전

- 뮤텍스 우선 순위 상속

각 뮤텍스의 다음에 대한 총 수:

  - 뮤텍스 put

  - 뮤텍스 get

  - 뮤텍스 get 일시 중단

  - 뮤텍스 get 시간 제한

  - 뮤텍스 우선 순위 반전

  - 뮤텍스 우선 순위 상속

이 정보는 ***tx_mutex_performance_info_get** _ 및 _*_tx_mutex_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 뮤텍스 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 "뮤텍스 get 시간 제한"의 수가 상대적으로 높으면 다른 스레드가 리소스를 너무 오래 보유하고 있다는 표시일 수 있습니다.

### <a name="mutex-control-block-tx_mutex"></a>뮤텍스 제어 블록 TX_MUTEX

각 뮤텍스의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 뮤텍스를 소유한 스레드의 포인터와 함께 현재 뮤텍스 소유권 수와 같은 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다. 뮤텍스 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="deadly-embrace"></a>치명적인 수용

뮤텍스 소유권과 관련된 가장 흥미롭고 위험한 문제 중 하나는 치명적인 수용(*deadly embrace*)입니다. 치명적인 수용 또는 *교착* 상태는 이미 서로를 소유하고 있는 뮤텍스를 get하려고 시도하는 동안 둘 이상의 스레드가 무기한 일시 중단되는 조건입니다. *치명적인 수용* 과 그 해결책에 대한 설명은 뮤텍스 개체에도 완전히 유효합니다.

### <a name="priority-inversion"></a>우선 순위 반전

앞서 언급했듯이 상호 배제와 관련된 주요 문제는 우선 순위 반전입니다. 이 항목은 "[스레드 우선 순위 관련 문제](#thread-priority-pitfalls)"에서 자세히 설명되어 있습니다.

기본적인 문제는 우선 순위가 높은 스레드에 필요한 세마포가 우선 순위가 낮은 스레드에 있는 상황에서 발생합니다. 이것 자체로는 정상입니다. 하지만 스레드의 우선 순위가 그 사이이면 우선 순위 반전이 비결정적 시간 동안 지속될 수 있습니다. 앞서 언급한 세마포와 달리 ThreadX 뮤텍스 개체에는 선택적 *우선 순위 상속* 이 있습니다. 우선 순위 상속의 기본 개념은 우선 순위가 낮은 스레드의 우선 순위가 우선 순위가 낮은 스레드가 소유한 것과 동일한 뮤텍스를 원하는 우선 순위가 높은 스레드의 우선 순위로 일시적으로 높아지는 것입니다. 우선 순위가 낮은 스레드가 뮤텍스를 해제하면 원래 우선 순위가 복원되고 우선 순위가 높은 스레드에 뮤텍스의 소유권이 부여됩니다. 이 기능은 반전의 크기를 우선 순위가 낮은 스레드가 뮤텍스를 보유하는 시간으로 제한하여 비결정적 우선 순위 반전을 제거합니다. 물론 이 챕터의 앞부분에서 언급한 비결정적 우선 순위 반전을 처리하는 내용은 뮤텍스에서도 유효합니다.

## <a name="event-flags"></a>이벤트 플래그

이벤트 플래그는 스레드 동기화를 위한 강력한 도구를 제공합니다. 각 이벤트 플래그는 단일 비트로 표시됩니다. 이벤트 플래그는 32개의 그룹으로 정렬됩니다. 한 그룹의 32개 이벤트 플래그 모두에서 스레드가 동시에 작동할 수 있습니다. 이벤트는 ***tx_event_flags_set** _에 의해 설정되고 _*_tx_event_flags_get_**에 의해 검색됩니다.

이벤트 플래그 설정은 현재 이벤트 플래그와 새 이벤트 플래그 사이의 논리적 AND/OR 연산을 사용하여 수행됩니다. 논리 연산 유형(AND 또는 OR)은 ***tx_event_flags_set*** 호출에 지정됩니다.

이벤트 플래그 검색을 위한 유사한 논리적 옵션이 있습니다. get 요청은 지정된 모든 이벤트 플래그가 필요하도록 지정할 수 있습니다(논리적 AND).

또는 get 요청은 지정된 이벤트 플래그 중 하나가 요청을 충족하도록 지정할 수 있습니다(논리적 OR). 이벤트 플래그 검색과 관련된 논리 연산 유형은 ***tx_event_flags_get*** 호출에 지정됩니다.

> [!IMPORTANT]
> *get 요청을 충족하는 이벤트 플래그가 사용됩니다. 즉,* **TX_OR_CLEAR***또는***TX_AND_CLEAR***가 요청에 의해 지정된 경우 0으로 설정됩니다.*

각 이벤트 플래그 그룹은 공용 리소스입니다. ThreadX는 이벤트 플래그 그룹이 사용되는 방식에 제약 조건을 두지 않습니다.

### <a name="creating-event-flags-groups"></a>이벤트 플래그 그룹 만들기

이벤트 플래그 그룹은 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 생성 시 그룹의 모든 이벤트 플래그는 0으로 설정됩니다. 애플리케이션의 이벤트 플래그 그룹 수에는 제한이 없습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

그룹에서 이벤트 플래그의 논리적 조합을 가져오려 시도하는 동안 애플리케이션 스레드가 일시 중단될 수 있습니다. 이벤트 플래그가 설정되면 일시 중단된 모든 스레드의 get 요청이 검토됩니다. 이제 필요한 이벤트 플래그가 있는 모든 스레드가 재개됩니다.

> [!NOTE]
> *이벤트 플래그 그룹에서 일시 중단된 모든 스레드는 해당 이벤트 플래그가 설정될 때 검토됩니다. 물론 추가 오버 헤드가 발생합니다. 따라서 동일한 이벤트 플래그 그룹을 사용하는 스레드 수를 적절한 수로 제한하는 것이 좋습니다.*

### <a name="event-flags-set-notification"></a>이벤트 플래그 설정 알림

일부 애플리케이션은 이벤트 플래그가 설정될 때마다 알림을 받는 것이 유리할 수 있습니다. ThreadX는 ***tx_event_flags_set_notify*** 서비스를 통해 이 기능을 제공합니다. 이 서비스는 제공된 애플리케이션 알림 함수를 지정된 이벤트 플래그 그룹에 등록합니다. ThreadX는 그룹의 이벤트 플래그가 설정될 때마다 애플리케이션 알림 함수를 호출합니다. 애플리케이션 알림 함수 내에서 정확한 처리는 애플리케이션에 의해 결정됩니다. 하지만 일반적으로 새 이벤트 플래그를 처리하기 위한 적절한 스레드를 재개하는 것으로 구성됩니다.

### <a name="event-flags-event-chainingtrade"></a>Event Flags Event chaining&trade;

ThreadX의 알림 기능을 사용하여 다양한 동기화 이벤트를 함께 "연결"할 수 있습니다. 일반적으로 단일 스레드가 여러 동기화 이벤트를 처리해야 하는 경우에 유용 합니다.

예를 들어 큐 메시지, 이벤트 플래그, 세마포에 대한 개별 스레드를 일시 중단하는 대신 애플리케이션은 각 개체에 대한 알림 루틴을 등록할 수 있습니다. 호출되면 애플리케이션 알림 루틴은 단일 스레드를 재개할 수 있습니다. 그러면 각 개체를 조사하여 새 이벤트를 찾아서 처리할 수 있습니다.

일반적으로 이벤트 연결(*event-chaining*)은 스레드 수, 오버헤드, RAM 요구 사항을 줄입니다. 또한 복잡한 시스템의 동기화 요구 사항을 처리할 수 있는 매우 유연한 메커니즘을 제공합니다.

### <a name="run-time-event-flags-performance-information"></a>런타임 이벤트 플래그 성능 정보

ThreadX는 선택적 런타임 이벤트 플래그 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_EVENT_FLAGS_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 이벤트 플래그 설정

  - 이벤트 플래그 get

  - 이벤트 플래그 get 일시 중단

  - 이벤트 플래그 get 시간 제한

각 이벤트 플래그 그룹의 다음에 대한 총 수:

  - 이벤트 플래그 설정

  - 이벤트 플래그 get

  - 이벤트 플래그 get 일시 중단

  - 이벤트 플래그 get 시간 제한

이 정보는 ***tx_event_flags_performance_info_get** _ 및 _*_tx_event_flags_performance_system_info_get_*_ 서비스를 통해 런타임에 사용할 수 있습니다. 이벤트 플래그 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 _ *_tx_event_flags_get_** 서비스의 시간 제한 수가 상대적으로 높으면 이벤트 플래그 일시 중단 제한 시간이 너무 짧다는 표시일 수 있습니다.

### <a name="event-flags-group-control-block-tx_event_flags_group"></a>이벤트 플래그 그룹 제어 블록 TX_EVENT_FLAGS_GROUP

각 이벤트 플래그 그룹의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 현재 이벤트 플래그 설정 및 이벤트에 대해 일시 중단된 스레드 수와 같은 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

이벤트 그룹 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="memory-block-pools"></a>메모리 블록 풀

실시간 애플리케이션에서 빠르고 결정적인 방식으로 메모리를 할당하는 것은 항상 어렵습니다. 이 점을 고려하여, ThreadX는 다수의 고정 크기 메모리 블록 풀을 만들고 관리하는 기능을 제공합니다.

메모리 블록 풀은 고정 크기 블록으로 구성되기 때문에 조각화 문제가 전혀 없습니다. 물론, 조각화는 본질적으로 비결정적 동작을 유발합니다. 또한 고정 크기의 메모리 블록을 할당하고 해제하는 데 필요한 시간은 간단한 연결된 목록을 조작하는 시간과 유사 합니다. 또한 메모리 블록 할당 및 할당 취소는 사용 가능한 목록의 헤드에서 수행됩니다. 이렇게 하면 가능한 가장 빠른 연결된 목록 처리가 제공되며 실제 메모리 블록을 캐시에 유지하는 데 유용할 수 있습니다.

유연성 부족은 고정 크기 메모리 풀의 주요 단점입니다. 풀의 블록 크기는 사용자에 대한 최악의 경우 메모리 요구 사항을 처리할 수 있을 만큼 충분히 커야 합니다. 물론 동일한 풀에 서로 다른 크기의 메모리 요청이 많이 발생하면 메모리가 낭비될 수 있습니다. 가능한 해결 방법은 서로 다른 크기의 메모리 블록을 포함하는 몇 가지 다른 메모리 블록 풀을 만드는 것입니다.

각 메모리 블록 풀은 공용 리소스입니다. ThreadX는 풀이 사용되는 방식에 제약 조건을 두지 않습니다.

### <a name="creating-memory-block-pools"></a>메모리 블록 풀 만들기

메모리 블록 풀은 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 애플리케이션의 메모리 블록 풀 수에는 제한이 없습니다.

### <a name="memory-block-size"></a>메모리 블록 크기

앞서 언급했듯이 메모리 블록 풀에는 여러 고정 크기 블록이 포함됩니다. 블록 크기(바이트 단위)는 풀을 만드는 동안 지정됩니다.

> [!NOTE]
> *ThreadX는 풀의 각 메모리 블록에 적은 양의 오버헤드(C 포인터의 크기)를 추가합니다. 또한 ThreadX는 각 메모리 블록의 시작을 적절한 정렬로 유지하기 위해 블록 크기를 채워야 할 수 있습니다.*

### <a name="pool-capacity"></a>풀 용량

풀의 메모리 블록 수는 블록 크기 및 생성 중에 제공된 메모리 영역의 총 바이트 수의 함수입니다. 풀 용량은 블록 크기(패딩 및 포인터 오버헤드 바이트 포함)를 제공된 메모리 영역의 총 바이트 수로 나누어 계산합니다.

### <a name="pools-memory-area"></a>풀의 메모리 영역

앞서 언급했듯이 블록 풀의 메모리 영역은 생성 중에 지정됩니다. ThreadX의 다른 메모리 영역과 마찬가지로 대상 주소 공간의 어디에나 위치할 수 있습니다.

이것은 상당한 유연성을 제공하기 때문에 중요한 기능입니다. 예를 들어, 통신 제품에 I/O를 위한 고속 메모리 영역이 있다고 가정합니다. 이 메모리 영역을 ThreadX 메모리 블록 풀로 만들어서 쉽게 관리할 수 있습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

빈 풀에서 메모리 블록을 기다리는 동안 애플리케이션 스레드가 일시 중단될 수 있습니다. 블록이 풀로 반환되면 일시 중단된 스레드에 이 블록이 주어지고 스레드가 재개됩니다.

여러 스레드가 동일한 메모리 블록 풀에서 일시 중단되면 일시 중단된 순서대로(FIFO) 재개됩니다.

하지만 스레드 일시 중단을 해제하는 블록 릴리스 호출 전에 애플리케이션이 ***tx_block_pool_prioritize*** 를 호출하는 경우에도 우선 순위 재개가 가능합니다. 블록 풀 우선 순위 지정 서비스는 우선 순위가 가장 높은 스레드를 일시 중단 목록의 맨 앞에 배치하고 다른 모든 일시 중단된 스레드는 동일한 FIFO 순서를 유지합니다.

### <a name="run-time-block-pool-performance-information"></a>런타임 블록 풀 성능 정보

ThreadX는 선택적 런타임 블록 풀 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_BLOCK_POOL_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 할당된 블록

  - 해제된 블록

  - 할당 일시 중단

  - 할당 시간 제한

각 블록 풀의 다음에 대한 총 수:

  - 할당된 블록

  - 해제된 블록

  - 할당 일시 중단

  - 할당 시간 제한

이 정보는 ***tx_block_pool_performance_info_get** _ 및 _*_tx_block_pool_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 블록 풀 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 "할당 일시 중단"의 수가 상대적으로 높으면 블록 풀이 너무 작다는 표시일 수 있습니다.

### <a name="memory-block-pool-control-block-tx_block_pool"></a>메모리 블록 풀 제어 블록 TX_BLOCK_POOL

각 메모리 블록 풀의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 사용 가능한 메모리 블록 수 및 메모리 풀 블록 크기와 같은 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

풀 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="overwriting-memory-blocks"></a>메모리 블록 덮어쓰기

할당된 메모리 블록의 사용자가 경계 외부에 쓰지 않도록 하는 것이 중요합니다. 만약 그렇게 되면 인접한(일반적으로 후속) 메모리 영역에서 손상이 발생합니다. 결과는 예측할 수 없으며, 애플리케이션에 치명적인 경우가 많습니다.

## <a name="memory-byte-pools"></a>메모리 바이트 풀

ThreadX 메모리 바이트 풀은 표준 C 힙과 비슷합니다. 표준 C 힙과 달리 여러 메모리 바이트 풀을 포함할 수 있습니다. 또한 요청된 메모리를 사용할 수 있을 때까지 스레드가 풀에서 일시 중단될 수 있습니다.

메모리 바이트 풀의 할당은 기존 ***malloc** _ 호출과 유사하며 여기에는 원하는 메모리 크기(바이트)가 포함됩니다. 메모리는 최초 적합(_first-fit*) 방식으로 풀에서 할당됩니다. 즉, 요청을 충족하는 첫 번째 사용 가능 메모리 블록이 사용됩니다. 이 블록에서 초과하는 메모리는 새 블록으로 변환되어 사용 가능한 메모리 목록에 다시 배치됩니다. 이 프로세스를 조각화(*fragmentation*)라고 합니다.

사용 가능한 인접 메모리 블록은 충분히 큰 여유 메모리 블록에 대한 후속 할당 검색 중에 함께 *병합* 됩니다. 이 프로세스를 조각 모음(*defragmentation*)이라고 합니다.

각 메모리 바이트 풀은 공용 리소스입니다. ThreadX는 ISR에서 메모리 바이트 서비스를 호출할 수 없다는 점을 제외하고, 풀이 사용되는 방식에 제약 조건을 두지 않습니다.

### <a name="creating-memory-byte-pools"></a>메모리 바이트 풀 만들기

메모리 바이트 풀은 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 애플리케이션의 메모리 바이트 풀 수에는 제한이 없습니다.

### <a name="pool-capacity"></a>풀 용량

메모리 바이트 풀에서 할당 가능한 바이트 수는 생성 중에 지정된 것보다 약간 적습니다. 여유 메모리 영역을 관리하면 약간의 오버헤드가 발생하기 때문입니다. 풀의 사용 가능한 각 메모리 블록에는 오버헤드의 두 C 포인터에 해당하는 것이 필요합니다. 또한 풀은 두 개의 블록, 즉 큰 여유 블록과 메모리 영역 끝에 영구 할당된 작은 블록으로 만들어집니다. 이렇게 할당된 블록은 할당 알고리즘의 성능을 높이는 데 사용됩니다. 병합하는 동안 풀 영역의 끝을 지속적으로 확인하지 않아도 됩니다.

런타임 중에는 풀의 오버헤드 크기가 일반적으로 증가합니다. 다음 메모리 블록의 적절한 정렬을 보장하기 위해 홀수 바이트 할당이 채워집니다. 또한 풀 조각화가 더 진행될수록 오버헤드는 증가합니다.

### <a name="pools-memory-area"></a>풀의 메모리 영역

메모리 바이트 풀의 메모리 영역은 생성 중에 지정됩니다. ThreadX의 다른 메모리 영역과 마찬가지로 대상 주소 공간의 어디에나 위치할 수 있습니다. 이것은 상당한 유연성을 제공하기 때문에 중요한 기능입니다. 예를 들어 대상 하드웨어에 고속 메모리 영역과 저속 메모리 영역이 있는 경우 사용자는 각 영역에 풀을 생성하여 두 영역에 대한 메모리 할당을 관리할 수 있습니다.

### <a name="thread-suspension"></a>스레드 일시 중단

풀의 메모리 바이트를 기다리는 동안 애플리케이션 스레드가 일시 중단될 수 있습니다. 충분한 연속 메모리를 사용할 수 있게 되면 일시 중단된 스레드에 요청된 메모리가 제공되고 스레드가 재개됩니다.

여러 스레드가 동일한 메모리 바이트 풀에서 일시 중단되면 일시 중단된 순서대로(FIFO) 메모리가 주어집니다.

하지만 스레드 일시 중단을 해제하는 바이트 릴리스 호출 전에 애플리케이션이 ***tx_byte_pool_prioritize*** 를 호출하는 경우에도 우선 순위 재개가 가능합니다. 바이트 풀 우선 순위 지정 서비스는 우선 순위가 가장 높은 스레드를 일시 중단 목록의 맨 앞에 배치하고 다른 모든 일시 중단된 스레드는 동일한 FIFO 순서를 유지합니다.

### <a name="run-time-byte-pool-performance-information"></a>런타임 바이트 풀 성능 정보

ThreadX는 선택적 런타임 바이트 풀 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 ***TX_BYTE_POOL_ENABLE_PERFORMANCE_INFO*** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

  - 할당

  - 릴리스

  - 검색된 조각

  - 병합된 조각

  - 생성된 조각

  - 할당 일시 중단

  - 할당 시간 제한

각 바이트 풀의 다음에 대한 총 수:

  - 할당

  - 릴리스

  - 검색된 조각

  - 병합된 조각

  - 생성된 조각

  - 할당 일시 중단

  - 할당 시간 제한

이 정보는 ***tx_byte_pool_performance_info_get** _ 및 _*_tx_byte_pool_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 바이트 풀 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다. 예를 들어 "할당 일시 중단"의 수가 상대적으로 높으면 바이트 풀이 너무 작다는 표시일 수 있습니다.

### <a name="memory-byte-pool-control-block-tx_byte_pool"></a>메모리 바이트 풀 제어 블록 TX_BYTE_POOL

각 메모리 바이트 풀의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 풀에서 사용 가능한 바이트 수와 같은 유용한 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

풀 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="nondeterministic-behavior"></a>비결정적 동작

메모리 바이트 풀은 가장 유연한 메모리 할당을 제공하지만 다소 비결정적 동작으로 인해 어려움이 있습니다. 예를 들어, 메모리 바이트 풀은 2,000바이트의 사용 가능 메모리를 가질 수 있지만 1,000 바이트의 할당 요청을 충족하지 못할 수 있습니다. 사용 가능한 바이트 중 얼마나 많은 바이트가 연속 바이트인지에 대한 보장이 없기 때문입니다. 1,000바이트의 여유 블록이 있어도 블록을 찾는 데 시간이 얼마나 걸릴지에 대한 보장은 없습니다. 1,000바이트 블록을 찾기 위해 전체 메모리 풀을 검색해야 할 수도 있습니다.

> [!TIP]
> *메모리 바이트 풀의 비결정적 동작의 결과로, 일반적으로 결정적이며 실시간 동작이 필요한 영역에서는 메모리 바이트 서비스를 사용하지 않는 것이 좋습니다. 많은 애플리케이션이 초기화 또는 런타임 구성 중에 필요한 메모리를 미리 할당합니다.*

### <a name="overwriting-memory-blocks"></a>메모리 블록 덮어쓰기

할당된 메모리의 사용자가 경계 외부에 쓰지 않도록 하는 것이 중요합니다. 만약 그렇게 되면 인접한(일반적으로 후속) 메모리 영역에서 손상이 발생합니다. 결과는 예측할 수 없으며, 프로그램 실행에 치명적인 경우가 많습니다.

## <a name="application-timers"></a>애플리케이션 타이머

비동기 외부 이벤트에 대한 빠른 응답은 내장된 실시간 애플리케이션의 가장 중요한 기능입니다. 하지만 이러한 애플리케이션은 대부분 미리 결정된 시간 간격으로 특정 작업을 수행해야 합니다.

ThreadX 애플리케이션 타이머는 애플리케이션에 특정 시간 간격으로 애플리케이션 C 함수를 실행할 수 있는 기능을 제공합니다. 애플리케이션 타이머가 한 번만 만료될 수도 있습니다. 이러한 형식의 타이머를 원샷 타이머(*one-shot timer*)라고 하고 반복 간격 타이머는 주기적 타이머(*periodic timers*)라고 합니다.

각 애플리케이션 타이머는 공용 리소스입니다. ThreadX는 애플리케이션 타이머가 사용되는 방식에 제약 조건을 두지 않습니다.

### <a name="timer-intervals"></a>타이머 간격

ThreadX에서 시간 간격은 주기적인 타이머 인터럽트로 측정됩니다. 각 타이머 인터럽트를 타이머 *틱* 이라고 합니다. 타이머 틱 사이의 실제 시간은 애플리케이션에서 지정되지만 대부분의 구현에서는 10ms입니다. 주기적 타이머 설정은 일반적으로 ***tx_initialize_low_level*** 어셈블리 파일에 있습니다.

기본 하드웨어는 애플리케이션 타이머가 작동하기 위해 주기적인 인터럽트를 생성할 수 있어야 합니다. 프로세서에 주기적인 인터럽트 기능이 기본 제공되는 경우가 있습니다. 프로세서에 이 기능이 없으면 사용자 보드에 주기적인 인터럽트를 생성할 수 있는 주변 장치가 있어야 합니다.

> [!IMPORTANT]
> *ThreadX는 주기적인 인터럽트 원본 없이도 계속 작동할 수 있습니다. 그러나 모든 타이머 관련 처리는 사용 하지 않도록 설정됩니다. 여기에는 시간 조각화, 일시 중단 시간 제한 및 타이머 서비스가 포함됩니다.*

### <a name="timer-accuracy"></a>타이머 정확도

타이머 만료는 틱 단위로 지정됩니다. 지정된 만료 값은 타이머 틱마다 1씩 감소합니다. 타이머 인터럽트(또는 타이머 틱) 직전에 애플리케이션 타이머를 사용하도록 설정할 수 있기 때문에 실제 만료 시간은 최대 1틱이 빠를 수 있습니다.

타이머 틱 속도가 10ms이면 애플리케이션 타이머는 최대 10ms 일찍 만료될 수 있습니다. 이것은 1초 타이머보다는 10ms 타이머에서 더 중요합니다. 물론 타이머 인터럽트 빈도를 높이면 이러한 오차 범위가 줄어듭니다.

### <a name="timer-execution"></a>타이머 실행

애플리케이션 타이머는 활성 상태가 되는 순서대로 실행됩니다. 예를 들어 동일한 만료 값으로 세 개의 타이머를 생성하고 활성화하면 해당 만료 함수는 활성화된 순서대로 실행되도록 보장됩니다.

### <a name="creating-application-timers"></a>애플리케이션 타이머 만들기

애플리케이션 타이머는 초기화 중에 또는 애플리케이션 스레드에서 런타임 중에 생성됩니다. 애플리케이션의 애플리케이션 타이머 수에는 제한이 없습니다.

### <a name="run-time-application-timer-performance-information"></a>런타임 애플리케이션 타이머 성능 정보

ThreadX는 선택적 런타임 애플리케이션 타이머 성능 정보를 제공합니다. ThreadX 라이브러리 및 애플리케이션이 **TX_TIMER_ENABLE_PERFORMANCE_INFO** 가 정의된 상태로 빌드되면 ThreadX는 다음 정보를 누적합니다.

전체 시스템의 다음에 대한 총 수:

- 활성화

- 비활성화

- 재활성화(주기적 타이머)

- 만료

- 만료 조정

각 애플리케이션 타이머의 다음에 대한 총 수:

- 활성화

- 비활성화

- 재활성화(주기적 타이머)

- 만료

- 만료 조정

이 정보는 ***tx_timer_performance_info_get** _ 및 _*_tx_timer_performance_system_info_get_** 서비스를 통해 런타임에 사용할 수 있습니다. 애플리케이션 타이머 성능 정보는 애플리케이션이 제대로 작동하는지 확인하는 데 유용합니다. 애플리케이션을 최적화하는데도 유용합니다.

### <a name="application-timer-control-block-tx_timer"></a>애플리케이션 타이머 제어 블록 TX_TIMER

각 애플리케이션 타이머의 특징은 해당 제어 블록에서 찾을 수 있습니다. 여기에는 32비트 만료 식별 값과 같은 유용한 정보가 포함됩니다. 이 구조는 ***tx_api.h*** 파일에 정의되어 있습니다.

애플리케이션 타이머 제어 블록은 메모리의 어디에나 위치할 수 있지만 제어 블록을 모든 함수의 범위 밖에 정의하여 전역 구조로 만드는 것이 가장 일반적입니다.

### <a name="excessive-timers"></a>과도한 타이머

기본적으로 애플리케이션 타이머는 우선 순위 0에서 실행되는 숨겨진 시스템 스레드 내에서 실행되며, 이것은 일반적으로 애플리케이션 스레드보다 높습니다. 따라서 애플리케이션 타이머 내부 처리는 최소한으로 유지되어야 합니다.

모든 타이머 틱이 만료되는 타이머를 가능하면 피하는 것도 중요합니다. 이러한 상황은 애플리케이션에서 과도한 오버헤드를 유발할 수 있습니다.

> [!IMPORTANT]
> *앞서 언급했듯이 애플리케이션 타이머는 숨겨진 시스템 스레드에서 실행됩니다. 따라서 애플리케이션 타이머의 만료 함수 내에서 만들어진 ThreadX 서비스 호출에서는 일시 중단을 선택하지 않는 것이 중요합니다.*

## <a name="relative-time"></a>상대 시간

앞서 언급한 애플리케이션 타이머 외에도 ThreadX는 지속적으로 증가하는 단일 32비트 틱 카운터를 제공합니다. 틱 카운터 또는 *시간* 은 각 타이머 인터럽트마다 1씩 증가합니다.

애플리케이션은 각각 ***tx_time_get** _ 및 _*_tx_time_set_**에 대한 호출을 통해 32비트 카운터를 읽거나 설정할 수 있습니다. 틱 카운터 사용은 완전히 애플리케이션에 의해 결정됩니다. ThreadX에서 내부적으로 사용되지 않습니다.

## <a name="interrupts"></a>인터럽트

비동기 이벤트에 대한 빠른 응답은 내장된 실시간 애플리케이션의 주요 기능입니다. 애플리케이션은 하드웨어 인터럽트를 통해 이러한 이벤트가 있다는 것을 알고 있습니다.

인터럽트는 프로세서 실행의 비동기 변경입니다. 일반적으로 *인터럽트* 가 발생하면 프로세서는 현재 실행의 작은 부분을 스택에 저장하고 적절한 인터럽트 벡터로 제어권을 이전합니다. 인터럽트 벡터는 기본적으로 특정 형식 인터럽트를 처리하는 루틴의 주소일 뿐입니다. 정확한 인터럽트 처리 절차는 프로세서마다 다릅니다.

### <a name="interrupt-control"></a>인터럽트 제어

***tx_interrupt_control*** 서비스를 사용하면 애플리케이션에서 인터럽트를 사용하거나 사용하지 않도록 설정할 수 있습니다. 이 서비스는 이전 인터럽트 사용/사용 안 함 상태를 반환합니다. 인터럽트 제어는 현재 실행 중인 프로그램 세그먼트에만 영향을 줍니다. 예를 들어 스레드가 인터럽트를 사용하지 않도록 설정하면 해당 스레드가 실행되는 동안만 사용하지 않도록 설정된 상태로 유지됩니다.

> [!NOTE]
> *마스크 불가능 인터럽트(NMI)는 하드웨어에서 비활성화할 수 없는 인터럽트입니다. 이러한 인터럽트는 ThreadX 애플리케이션에서 사용될 수 있습니다. 그러나 애플리케이션의 NMI 처리 루틴은 ThreadX 컨텍스트 관리 또는 API 서비스를 사용할 수 없습니다.*

### <a name="threadx-managed-interrupts"></a>ThreadX 관리형 인터럽트

ThreadX는 애플리케이션에 완전한 인터럽트 관리를 제공합니다. 이러한 관리에는 중단된 실행의 컨텍스트를 저장하고 복원하는 작업이 포함됩니다. 또한 ThreadX를 사용하면 ISR(인터럽트 서비스 루틴) 내에서 특정 서비스를 호출할 수 있습니다. 다음은 애플리케이션 ISR에서 허용되는 ThreadX 서비스 목록입니다.

```c
tx_block_allocate
tx_block_pool_info_get tx_block_pool_prioritize
tx_block_pool_performance_info_get
tx_block_pool_performance_system_info_get tx_block_release
tx_byte_pool_info_get tx_byte_pool_performance_info_get
tx_byte_pool_performance_system_info_get
tx_byte_pool_prioritize tx_event_flags_info_get
tx_event_flags_get tx_event_flags_set
tx_event_flags_performance_info_get
tx_event_flags_performance_system_info_get
tx_event_flags_set_notify tx_interrupt_control
tx_mutex_performance_info_get
tx_mutex_performance_system_info_get tx_queue_front_send
tx_queue_info_get tx_queue_performance_info_get
tx_queue_performance_system_info_get tx_queue_prioritize
tx_queue_receive tx_queue_send tx_semaphore_get
tx_queue_send_notify tx_semaphore_ceiling_put
tx_semaphore_info_get tx_semaphore_performance_info_get
tx_semaphore_performance_system_info_get
tx_semaphore_prioritize tx_semaphore_put tx_thread_identify
tx_semaphore_put_notify tx_thread_entry_exit_notify
tx_thread_info_get tx_thread_resume
tx_thread_performance_info_get
tx_thread_performance_system_info_get
tx_thread_stack_error_notify tx_thread_wait_abort tx_time_get
tx_time_set tx_timer_activate tx_timer_change
tx_timer_deactivate tx_timer_info_get
tx_timer_performance_info_get
tx_timer_performance_system_info_get
```

> [!IMPORTANT]
> *ISR에서는 일시 중단이 허용되지 않습니다. 따라서 ISR에서 수행된 모든 ThreadX 서비스 호출에 대한 **wait_option** 매개 변수를 **TX_NO_WAIT** 로 설정해야 합니다.*

### <a name="isr-template"></a>ISR 템플릿

애플리케이션 인터럽트를 관리하려면 애플리케이션 ISR의 시작과 끝에서 몇 개의 ThreadX 유틸리티를 호출해야 합니다. 인터럽트 처리의 정확한 형식은 포트마다 다릅니다.

다음 작은 코드 세그먼트는 대부분의 ThreadX 관리형 ISR에서 일반적입니다. 대부분의 경우 이러한 처리는 어셈블리 언어로 되어 있습니다.

```c
_application_ISR_vector_entry:

; Save context and prepare for

; ThreadX use by calling the ISR

; entry function.

CALL _tx_thread_context_save

; The ISR can now call ThreadX

; services and its own C functions

; When the ISR is finished, context

; is restored (or thread preemption)

; by calling the context restore ; function. Control does not return!

JUMP _tx_thread_context_restore
```

### <a name="high-frequency-interrupts"></a>빈도가 높은 인터럽트

일부 인터럽트는 매우 높은 빈도로 발생하여 인터럽트가 발생할 때마다 전체 컨텍스트를 저장하고 복원하면 과도한 처리 대역폭이 소비됩니다. 이런 경우 빈도가 높은 인터럽트의 대부분에 대해 제한된 크기의 처리를 수행하는 작은 어셈블리 언어 ISR을 애플리케이션에 두는 것이 일반적입니다.

특정 시점이 지나면 작은 ISR이 ThreadX와 상호 작용해야 할 수 있습니다. 이 작업은 위의 템플릿에 설명된 엔트리 및 종료 함수를 호출하여 수행됩니다.

### <a name="interrupt-latency"></a>인터럽트 대기 시간

ThreadX는 짧은 시간 동안 인터럽트를 잠급니다. 인터럽트를 사용하지 않도록 설정되는 최대 시간은 스레드의 컨텍스트를 저장하거나 복원하는 데 필요한 시간 순서입니다.
